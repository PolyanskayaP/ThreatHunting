---
title: "lab6"
author: "Полянская Полина Алексеевна"
format:   
  md:
    output-file: README.md
editor_options: 
  chunk_output_type: inline
---

Исследование вредоносной активности в домене
Windows

## Цель

1. Закрепить навыки исследования данных журнала Windows Active Directory
2. Изучить структуру журнала системы Windows Active Directory
3. Закрепить практические навыки использования языка программирования R для
обработки данных
4. Закрепить знания основных функций обработки данных экосистемы tidyverse
языка R


## Исходные данные

1.  Ноутбук с ОС Windows 10
2.  RStudio
3.  Пакеты dplyr

## Задание

Используя программный пакет dplyr языка
программирования R провести анализ журналов и
ответить на вопросы.

## Подготовка данных

```{r}
library(dplyr)
```
```{r}
library(jsonlite)
```
```{r}
library(tidyr)
```
```{r}
library(xml2)
```
```{r}
library(rvest)
```


### 1. Импортируйте данные в R. Это можно выполнить с помощью jsonlite::stream_in(file()) . Датасет находится по адресу https://storage.yandexcloud.net/iamcth-data/dataset.tar.gz.

```{r}
url_yand <- "https://storage.yandexcloud.net/iamcth-data/dataset.tar.gz"
download.file(url_yand, destfile = tf <- tempfile(fileext = ".tar.gz"), mode = "wb")#сохранение во временный файл
temp_dir <- tempdir()
untar(tf, exdir = temp_dir)
json_files <- list.files(temp_dir, pattern="\\.json$", full.names = TRUE, recursive = TRUE)
df <- stream_in(file(json_files))
```

```{r}
#fn <- "https://storage.yandexcloud.net/iamcth-data/dataset.tar.gz"
#download.file(fn,destfile="tmp.tar.gz")
```
```{r}
#untar("tmp.tar.gz")
```

```{r}
#json_str <- readr::read_file("caldera_attack_evals_round1_day1_2019-10-20201108.json")
```
```{r}
#my_df <- jsonlite::fromJSON(json_str)
```
-----------------
```{r}
#untar("dataset.tar.gz")
#df <- read_json('C:/ThreatHunting/lab6/caldera_attack_evals_round1_day1_2019-10-20201108.json')
#json_str <- readr::read_file(url("C:\\ThreatHunting\\lab6\\caldera_attack_evals_round1_day1_2019-10-20201108.json"))
```
```{r}
#install.packages("rjson",  dependencies = T)
```
```{r}
#library(rjson)
```
```{r}
#JsonData <- fromJSON(file = 'C:\\ThreatHunting\\lab6\\caldera_attack_evals_round1_day1_2019-10-20201108.json')
#print(JsonData)
```


### 2. Привести датасеты в вид “аккуратных данных”, преобразовать типы столбцов в соответствии с типом данных
```{r}
df %>% head()
```

```{r}
df <- df %>%
  mutate(`@timestamp` = as.POSIXct(`@timestamp`, format = "%Y-%m-%dT%H:%M:%OSZ", tz = "UTC")) %>%
  rename(timestamp = `@timestamp`, metadata = `@metadata`)
```

### 3. Просмотрите общую структуру данных с помощью функции glimpse()
```{r}
df %>% glimpse()
```

## Анализ

### 1. Раскройте датафрейм избавившись от вложенных датафреймов. Для обнаружения таких можно использовать функцию dplyr::glimpse() , а для раскрытия вложенности – tidyr::unnest() . Обратите внимание, что при раскрытии теряются внешние названия колонок – это можно предотвратить если использовать параметр tidyr::unnest(..., names_sep = ) .
```{r}
df$metadata %>% glimpse()
```
```{r}
df$event %>% glimpse()
```
```{r}
df$log %>% glimpse()
```

```{r}
df$winlog %>% glimpse()
```

```{r}
df$ecs %>% glimpse()
```

```{r}
df$host %>% glimpse()
```

```{r}
df$agent %>% glimpse()
```
```{r}
raskr_df <- df %>%
  unnest(c(metadata, event, log, winlog, ecs, host, agent), names_sep = ".") #для раскрытия вложенности – tidyr::unnest()
raskr_df %>% glimpse
```


### 2. Минимизируйте количество колонок в датафрейме – уберите колоки с единственным значением параметра.
```{r}
minimized_df <- raskr_df %>%
  select(-metadata.beat, -metadata.type, -metadata.version, -metadata.topic, -event.kind, -winlog.api, -agent.ephemeral_id, -agent.hostname, -agent.id, -agent.version, -agent.type) #select с минусом не берет столбцы
minimized_df %>% glimpse
```

### 3. Какое количество хостов представлено в данном датасете?

```{r}
uniq_host <- raskr_df %>% select(agent.hostname) %>% unique 
kolvo_host = count(uniq_host, "agent.hostname")
kolvo_host
```

### 4. Подготовьте датафрейм с расшифровкой Windows Event_ID, приведите типы данных к типу их значений.
```{r}
EventID_page <- xml2::read_html("https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/appendix-l--events-to-monitor")
df_EventID <- rvest::html_table(EventID_page)[[1]]
df_EventID %>% glimpse
```
```{r}
df_EventID <- df_EventID %>% mutate_at(vars(`Legacy Windows Event ID`,`Current Windows Event ID`), as.integer) 
```
```{r}
df_EventID %>% glimpse()
```


### 5. Есть ли в логе события с высоким и средним уровнем значимости? Сколько их?
```{r}
group_pc <- df_EventID %>% group_by(`Potential Criticality`)  
kolvo_znach = count(group_pc, `Potential Criticality`)
kolvo_znach %>% filter(`Potential Criticality` == "High" | `Potential Criticality` == "Medium")
```

## Оценка результатов

Выполнена задача анализа Wi-Fi-трафика, найдено, как загружать датасеты из интернета напрямую в воспроизводимых отчетах.

## Вывод

Wi-Fi-трафик можно анализировать при помощи языка программирования R.

